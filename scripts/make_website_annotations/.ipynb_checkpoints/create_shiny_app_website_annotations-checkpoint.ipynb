{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94e51f7",
   "metadata": {},
   "source": [
    "# Import Libraries and define settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27285546",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "\n",
    "\n",
    "## Display all rows of pandas dataframes\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9249f",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22801bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "name: relative_transcript_abundance\n",
    "\n",
    "purpose: calculate relative transcript abundance\n",
    "\n",
    "input: a dataframe with a ref_gene_id column identifying the transcript gene of origin and a cov columns with \n",
    "the coverage for the transcripts.\n",
    "\n",
    "output: the same dataframe with a relative abundance column added\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def relative_transcript_abundance(df):\n",
    "    \n",
    "    first=True\n",
    "    \n",
    "    dff = df.copy()\n",
    "    \n",
    "    for col in dff.filter(regex='[0-9]_CPM').columns:\n",
    "        \n",
    "        col_gene_name = col.split(\"_CP\")[0] + \"_total_gene_CPM\"\n",
    "        col_relative_abundance = col.split(\"_CP\")[0] + \"_relative_abundance\"\n",
    "    \n",
    "        dff_sums = dff[[\"gene_id\", col]].groupby(\"gene_id\").sum()\n",
    "\n",
    "        dff_sums[col_gene_name] = dff_sums[col].copy()\n",
    "\n",
    "        dff_sums.drop(columns=col, inplace=True)\n",
    "\n",
    "        if first:\n",
    "            merged_dff = pd.merge(dff, dff_sums, how='inner', on=\"gene_id\")\n",
    "            merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
    "            \n",
    "        else:\n",
    "            merged_dff = pd.merge(merged_dff, dff_sums, how='inner', on=\"gene_id\")\n",
    "            merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
    "        \n",
    "        first=False\n",
    "        \n",
    "    for col_count in merged_dff.filter(regex='[0-9]_count').columns:\n",
    "        \n",
    "        col_gene_name = col_count.split(\"_count\")[0] + \"_total_gene_counts\"\n",
    "        \n",
    "        dff_sums = merged_dff[[\"gene_id\", col_count]].groupby(\"gene_id\").sum()\n",
    "        dff_sums[col_gene_name] = dff_sums[col_count].copy()\n",
    "    \n",
    "        \n",
    "        dff_sums.drop(columns=col_count, inplace=True)\n",
    "        \n",
    "        merged_dff = pd.merge(merged_dff, dff_sums, how='inner', on=\"gene_id\")\n",
    "        \n",
    "    merged_dff.fillna(value=0, inplace=True)\n",
    "        \n",
    "    return merged_dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd344794",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function name: fix_column_names\n",
    "\n",
    "purpose: Fixing the column names, making them smaller, informative, and consistent\n",
    "\n",
    "input: The raw counts dataframe for either genes or transcripts \n",
    "\n",
    "output: Same dataframe with improved column names\n",
    "'''\n",
    "\n",
    "def fix_column_names(df, is_gene=False):\n",
    "    \n",
    "    dff = df.copy()\n",
    "    \n",
    "    ## Check if this is a gene counts object\n",
    "    if is_gene:\n",
    "        \n",
    "        ## Get count column names and create list of new column names\n",
    "        dff[\"gene_id\"] = dff[dff.columns[0]]\n",
    "        dff.drop(columns=dff.columns[0], inplace=True)\n",
    "        \n",
    "        ## gene_id comes in as index for gene counts data, make it into the first column instead\n",
    "        cols = list(dff.columns)\n",
    "        cols = [cols[-1]] + cols[:-1]\n",
    "        dff = dff[cols]\n",
    "        dff.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        ## Define counts columns and initiate new_columns list\n",
    "        count_columns = dff.columns[1:].tolist()\n",
    "        list_new_names = [\"gene_id\"]\n",
    "    \n",
    "    ## If it is a transcript dataset\n",
    "    else:\n",
    "        ## Set count columns and create list of new names\n",
    "        count_columns = dff.columns[2:].tolist()\n",
    "        list_new_names = [\"transcript_id\", \"gene_id\"]\n",
    "    \n",
    "    ## Fix names one by one and add to list of new names\n",
    "    for col in count_columns:\n",
    "        col = col.split(\"_mapped\")[0] + \"_counts\"\n",
    "        list_new_names.append(col)\n",
    "    \n",
    "    ## Rename columns\n",
    "    dff.columns = list_new_names\n",
    "    \n",
    "    return dff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f7c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function name: parse_dff_columns\n",
    "\n",
    "purpose: parsing the last aggregate column of the gtf/gff3 into useful columns and cleaning non-relevant columns\n",
    "\n",
    "input: dataframe containining \"raw\" gtf/gff\n",
    "\n",
    "output: dataframe containing gtf with useful columns [\"gene_id\", \"transcript_id\", etc...]\n",
    "'''\n",
    "\n",
    "def parse_df_columns(df, is_ref=True, is_transcript=False, is_exon=False):\n",
    "    \n",
    "    dff = df.copy()\n",
    "\n",
    "    if is_ref:\n",
    "\n",
    "        ## Get gene ids\n",
    "        dff[\"gene_id\"] = dff[\"other\"].str.split('\";', expand=True)[0].str.extract(\"([^ \\\"]*$)\", expand=True)\n",
    "        \n",
    "        ## Get gene names\n",
    "        dff[\"gene_name\"] = dff[\"other\"].str.split(\"gene_name \\\"\", expand=True)[1].str.split('\\\";', expand=True)[0]\n",
    "        \n",
    "        ## Get get transcript biotype\n",
    "        dff[\"gene_biotype\"] = dff[\"other\"].str.split('gene_biotype \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "        \n",
    "        if is_transcript:\n",
    "            dff[\"transcript_id\"] = dff[\"other\"].str.split('transcript_id \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "            dff[\"transcript_biotype\"] = dff[\"other\"].str.split('transcript_biotype \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "        \n",
    "        if is_exon:\n",
    "            dff[\"transcript_id\"] = dff[\"other\"].str.split('transcript_id \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "            dff[\"transcript_biotype\"] = dff[\"other\"].str.split('transcript_biotype \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "            dff[\"exon_number\"] = dff[\"other\"].str.split('exon_number \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "\n",
    "            \n",
    "        ## Drop \"other\" column\n",
    "        dff.drop(columns=[\"other\", \"dot_1\", \"dot_2\"], inplace=True)\n",
    "        \n",
    "\n",
    "    else:\n",
    "\n",
    "        ## Get gene ids\n",
    "        dff[\"gene_id\"] = dff[\"other\"].str.split('\";', expand=True)[0].str.extract(\"([^ \\\"]*$)\", expand=True)\n",
    "\n",
    "        ## Get transcript ids\n",
    "        dff[\"transcript_id\"] = dff[\"other\"].str.split('transcript_id \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "\n",
    "        ## Get exon number\n",
    "        dff[\"exon_number\"] = dff[\"other\"].str.split('exon_number \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "\n",
    "        ## Drop \"other\" column\n",
    "        dff.drop(columns=[\"other\", \"dot_1\", \"dot_2\"], inplace=True)\n",
    "\n",
    "    for col in dff.columns:\n",
    "        dff.loc[dff[col].isnull(), col] = np.NaN\n",
    "        \n",
    "\n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056a1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function name: calculate_cpm\n",
    "\n",
    "purpose: Calculate CPM for the each sample given\n",
    "\n",
    "input: Counts dataset\n",
    "\n",
    "output: Counts dataset with CPM columns as well\n",
    "'''\n",
    "\n",
    "def calculate_cpm(df, is_gene=False):\n",
    "    \n",
    "    dff = df.copy()\n",
    "\n",
    "    ## Set count columns if dataframe is gene counts\n",
    "    if is_gene:\n",
    "        count_columns = dff.columns[1:].tolist()\n",
    "    \n",
    "    ## Set count columns if dataframe is transcript counts\n",
    "    else:\n",
    "        count_columns = dff.columns[2:].tolist()\n",
    "\n",
    "    ## Loop through counts columns to calculate CPM and add to the dataframe\n",
    "    for col in count_columns:\n",
    "        \n",
    "        cpm_name = col.replace(\"_counts\", \"_CPM\")\n",
    "        dff[cpm_name] = ((dff[col].copy()/(dff[col].sum().copy())) * 1000000)\n",
    "    \n",
    "    return dff  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd58c4",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cddefead",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "\n",
    "## Open original reference\n",
    "original_ref = pd.read_csv(\"../../nextflow_pipeline/references/Homo_sapiens.GRCh38.109.gtf\", header=None, delimiter=\"\\t\", low_memory=False, \n",
    "                       names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"], comment=\"#\")\n",
    "\n",
    "\n",
    "## Bambu reference with novel and annotated transcripts\n",
    "bambu_ref = pd.read_csv(\"../../data/raw/GRCh38_quant_mapq10_gtf_109_and_high-confidence_GTEx_DATA/bambu_quant/extended_annotations.gtf\", header=None, delimiter=\"\\t\",\n",
    "                        low_memory=False, names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"], comment=\"#\")\n",
    "\n",
    "\n",
    "## New Transcript Events\n",
    "df_events = pd.read_csv(\"../../references/novel_events.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "## Bambu counts matrix\n",
    "df_gtex = pd.read_csv(\"../../data/raw/GRCh38_quant_mapq10_gtf_109_and_high-confidence_GTEx_DATA/bambu_quant/counts_transcript.txt\", \n",
    "                           delimiter=\"\\t\", low_memory=False, header=0)\n",
    "\n",
    "\n",
    "## Bambu counts matrix\n",
    "df_ours = pd.read_csv(\"../../data/raw/GRCh38_quant_mapq10_gtf_109_and_high-confidence_OUR_DATA/bambu_quant/counts_transcript.txt\", \n",
    "                           delimiter=\"\\t\", low_memory=False, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64bff1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all transcript IDs from ENSEMBL 96 GTF\n",
    "\n",
    "df_ensembl_96 = pd.read_csv(\"../../references/Homo_sapiens.GRCh38.96.gtf\", header=None, delimiter=\"\\t\",\n",
    "                        low_memory=False, names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"])\n",
    "\n",
    "df_ensembl_96 = df_ensembl_96.loc[df_ensembl_96[\"type\"] == \"transcript\"].copy()\n",
    "\n",
    "df_ensembl_96 = parse_df_columns(df_ensembl_96, is_ref=True, is_transcript=True)\n",
    "\n",
    "ensembl_96_transcript_ids = df_ensembl_96[\"transcript_id\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dad343",
   "metadata": {},
   "source": [
    "# Create a reference with relevant information for each transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa788c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse through original reference to get all needed info\n",
    "\n",
    "orig_ref_transcripts = original_ref.loc[original_ref[\"type\"]==\"transcript\"].copy()\n",
    "orig_ref_exons = original_ref.loc[((original_ref[\"type\"]==\"exon\") | (original_ref[\"type\"]==\"CDS\"))].copy()\n",
    "\n",
    "orig_ref_transcripts = parse_df_columns(orig_ref_transcripts, is_ref=True, is_transcript=True)\n",
    "orig_ref_exons = parse_df_columns(orig_ref_exons, is_ref=True, is_exon=True)\n",
    "\n",
    "orig_ref_transcripts.drop(columns=[\"source\"], inplace=True)\n",
    "orig_ref_transcripts[\"exon_number\"] = np.nan\n",
    "\n",
    "orig_ref_exons.drop(columns=[\"source\"], inplace=True)\n",
    "\n",
    "orig_ref = pd.concat([orig_ref_transcripts, orig_ref_exons]).sort_values(by=[\"chr\", \"start\", \"type\", \"end\"], \n",
    "                                                                                    ascending=[True, True, False, True])\n",
    "\n",
    "\n",
    "orig_ref.loc[orig_ref[\"transcript_id\"].isin(ensembl_96_transcript_ids), \"annotation_status\"] = \"Annotated in ENSEMBL 96 (2019)\"\n",
    "orig_ref.loc[~orig_ref[\"transcript_id\"].isin(ensembl_96_transcript_ids), \"annotation_status\"] = \"NOT annotated in ENSEMBL 96 (2019)\"\n",
    "\n",
    "orig_ref[\"discovery_category\"] = \"Annotated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e9b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup df_events\n",
    "df_events = df_events[[\"TXNAME\", \"txClassDescription\"]]\n",
    "\n",
    "## Cleanup net transcript classes\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newGene-spliced\", \"txClassDescription\"] = \"New Gene Body\"\n",
    "\n",
    "## Cleanup net transcript classes\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newWithin\", \"txClassDescription\"] = \"New combination of known exons/junctions\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newFirstJunction:newFirstExon\", \"txClassDescription\"] = \"New exon\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction:newLastExon\", \"txClassDescription\"] = \"New exon\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"allNew\", \"txClassDescription\"] = \"All new exons & junctions\"\n",
    "\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newFirstJunction\", \"txClassDescription\"] = \"New junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction:newJunction\", \"txClassDescription\"] = \"New junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newFirstJunction:newJunction\", \"txClassDescription\"] = \"New junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction\", \"txClassDescription\"] = \"New junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newJunction\", \"txClassDescription\"] = \"New junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction:newFirstJunction:newJunction\", \"txClassDescription\"] = \"New junction\"\n",
    "\n",
    "\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction:newJunction:newLastExon\", \"txClassDescription\"] = \"New exon & new junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newFirstJunction:newJunction:newFirstExon\", \"txClassDescription\"] = \"New exon & new junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction:newFirstJunction:newJunction:newFirstExon:newLastExon\", \"txClassDescription\"] = \"New exon & new junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction:newFirstJunction:newJunction:newFirstExon\", \"txClassDescription\"] = \"New exon & new junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"New first exon & new junction\", \"txClassDescription\"] = \"New exon & new junction\"\n",
    "df_events.loc[df_events[\"txClassDescription\"] == \"newLastJunction:newFirstJunction:newJunction:newLastExon\", \"txClassDescription\"] = \"New exon & new junction\"\n",
    "\n",
    "## Change column names\n",
    "df_events.columns = [\"transcript_id\", \"discovery_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a46c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse through bambu reference\n",
    "bambu_ref = parse_df_columns(bambu_ref, is_ref=False)\n",
    "\n",
    "## Only keep novel genes and Transcripts\n",
    "bambu_ref = bambu_ref.loc[bambu_ref[\"transcript_id\"].str.startswith(\"BambuTx\")]\n",
    "\n",
    "## Create discovery category column\n",
    "bambu_ref[\"annotation_status\"] = \"Unnanotated (newly discovered)\"\n",
    "bambu_ref[\"transcript_biotype\"] = \"Unnanotated (newly discovered)\"\n",
    "bambu_ref = bambu_ref.merge(df_events, on=\"transcript_id\", how=\"inner\").drop_duplicates()\n",
    "\n",
    "## Get gene names\n",
    "gene_names = original_ref.loc[original_ref[\"type\"]==\"gene\"].copy()\n",
    "gene_names = parse_df_columns(gene_names, is_ref=True)\n",
    "gene_names = gene_names[[\"gene_id\", \"gene_name\"]]\n",
    "\n",
    "## Add gene_name\n",
    "bambu_ref = bambu_ref.merge(gene_names, on=\"gene_id\", how=\"left\").drop_duplicates().drop(columns=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fb9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Reference\n",
    "final_ref = pd.concat([orig_ref, bambu_ref]).sort_values(by=[\"chr\", \"start\", \"type\", \"end\"], \n",
    "                                                                                    ascending=[True, True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc933f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix transcript biotypes\n",
    "\n",
    "## Create CDS_not_defined classification\n",
    "final_ref.loc[((final_ref[\"gene_biotype\"] == \"protein_coding\") & (final_ref[\"transcript_biotype\"] == \"processed_transcript\")), \"transcript_biotype\"] = \"cds_not_defined\"\n",
    "\n",
    "## Create other classification\n",
    "final_ref.loc[~final_ref[\"transcript_biotype\"].isin([\"protein_coding\", \"nonsense_mediated_decay\", \"lncRNA\", \"retained_intron\", \"cds_not_defined\", \"processed_transcript\", \"Unnanotated (newly discovered)\"]), \"transcript_biotype\"] = \"other\"\n",
    "\n",
    "## Drop gene_biotype column\n",
    "final_ref.drop(columns=\"gene_biotype\", inplace=True)\n",
    "\n",
    "## Only keep CDS regions for protein_coding transcripts\n",
    "final_ref_cds = final_ref.loc[final_ref[\"type\"] == \"CDS\"].copy()\n",
    "final_ref_cds_prot = final_ref_cds.loc[final_ref_cds[\"transcript_biotype\"] == \"protein_coding\"].copy()\n",
    "final_ref_2 = final_ref.loc[final_ref[\"type\"] != \"CDS\"].copy()\n",
    "final_final_ref = pd.concat([final_ref_2, final_ref_cds_prot]).sort_values(by=[\"chr\", \"start\", \"type\", \"end\"], \n",
    "                                                                                    ascending=[True, True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c192ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save files\n",
    "final_final_ref.to_csv(\"../../data/processed/website_annotations/annotation_r_shiny.tsv\", sep=\"\\t\", \n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce41809",
   "metadata": {},
   "source": [
    "# Create transcript level matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b9755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n",
      "/tmp/ipykernel_1347926/1552211404.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_dff[col_relative_abundance] = ((merged_dff[col]/merged_dff[col_gene_name]) * 100)\n"
     ]
    }
   ],
   "source": [
    "## Fix expression matrix\n",
    "df_gtex = fix_column_names(df_gtex, is_gene=False)\n",
    "df_gtex = calculate_cpm(df_gtex, is_gene=False)\n",
    "\n",
    "## Calculate relative abundance\n",
    "df_gtex = relative_transcript_abundance(df_gtex)\n",
    "\n",
    "## Save expression Matrix\n",
    "df_gtex.to_csv(\"../../data/processed/website_annotations/expression_matrix_GTEX_r_shiny.tsv\", sep=\"\\t\",\n",
    "         index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9690659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix expression matrix\n",
    "df_ours = fix_column_names(df_ours, is_gene=False)\n",
    "df_ours = calculate_cpm(df_ours, is_gene=False)\n",
    "\n",
    "## Calculate relative abundance\n",
    "df_ours = relative_transcript_abundance(df_ours)\n",
    "\n",
    "## Save expression Matrix\n",
    "df_ours.to_csv(\"../../data/processed/website_annotations/expression_matrix_OURS_r_shiny.tsv\", sep=\"\\t\",\n",
    "         index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7844e",
   "metadata": {},
   "source": [
    "# Create gene level matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b54d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create gene counts matrix with no introns\n",
    "\n",
    "## Bambu counts matrix\n",
    "df_gtex = pd.read_csv(\"../../data/raw/GRCh38_quant_mapq10_gtf_109_and_high-confidence_GTEx_DATA/bambu_quant/counts_transcript.txt\", \n",
    "                           delimiter=\"\\t\", low_memory=False, header=0)\n",
    "\n",
    "df_gtex = fix_column_names(df_gtex, is_gene=False)\n",
    "\n",
    "df_gene_gtex = df_gtex.drop(columns=\"transcript_id\").groupby(\"gene_id\").sum().reset_index()\n",
    "\n",
    "df_gene_gtex = calculate_cpm(df_gene_gtex, is_gene=True)\n",
    "\n",
    "counts_columns = df_gene_gtex.filter(regex='counts').columns.to_list()\n",
    "df_gene_gtex[\"median_counts\"] = df_gene_gtex[counts_columns].median(axis=1)\n",
    "\n",
    "df_gene_gtex.drop(columns=counts_columns, inplace=True)\n",
    "\n",
    "## Calculate CPM and give it proper name\n",
    "cpm_cols = df_gene_gtex.filter(regex='CPM').columns.to_list()\n",
    "df_gene_gtex[\"median_cpm\"] = df_gene_gtex[cpm_cols].median(axis=1)\n",
    "df_gene_gtex.drop(columns=cpm_cols, inplace=True)\n",
    "\n",
    "## Save file\n",
    "df_gene_gtex.to_csv(\"../../data/processed/website_annotations/expression_matrix_GTEx_r_shiny_GENE.tsv\",\n",
    "              sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c794a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create gene counts matrix with no introns\n",
    "\n",
    "## Bambu counts matrix\n",
    "df_ours = pd.read_csv(\"../../data/raw/GRCh38_quant_mapq10_gtf_109_and_high-confidence_OUR_DATA/bambu_quant/counts_transcript.txt\", \n",
    "                           delimiter=\"\\t\", low_memory=False, header=0)\n",
    "\n",
    "df_ours = fix_column_names(df_ours, is_gene=False)\n",
    "\n",
    "df_gene_ours = df_ours.drop(columns=\"transcript_id\").groupby(\"gene_id\").sum().reset_index()\n",
    "\n",
    "df_gene_ours = calculate_cpm(df_gene_ours, is_gene=True)\n",
    "\n",
    "counts_columns = df_gene_ours.filter(regex='counts').columns.to_list()\n",
    "df_gene_ours[\"median_counts\"] = df_gene_ours[counts_columns].median(axis=1)\n",
    "\n",
    "df_gene_ours.drop(columns=counts_columns, inplace=True)\n",
    "\n",
    "## Calculate CPM and give it proper name\n",
    "cpm_cols = df_gene_ours.filter(regex='CPM').columns.to_list()\n",
    "df_gene_ours[\"median_cpm\"] = df_gene_ours[cpm_cols].median(axis=1)\n",
    "df_gene_ours.drop(columns=cpm_cols, inplace=True)\n",
    "\n",
    "## Save file\n",
    "df_gene_ours.to_csv(\"../../data/processed/website_annotations/expression_matrix_OURS_r_shiny_GENE.tsv\",\n",
    "              sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454c8ca",
   "metadata": {},
   "source": [
    "# Create annotations to check for protein sequences for Maddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "826dce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open original reference\n",
    "original_ref = pd.read_csv(\"../../nextflow_pipeline/references/Homo_sapiens.GRCh38.109.gtf\", header=None, delimiter=\"\\t\", low_memory=False, \n",
    "                       names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"], comment=\"#\")\n",
    "\n",
    "\n",
    "\n",
    "## Bambu reference with novel and annotated transcripts\n",
    "bambu_ref = pd.read_csv(\"../../data/raw/GRCh38_quant_mapq10_gtf_109_and_high-confidence_GTEx_DATA/bambu_quant/extended_annotations.gtf\", header=None, delimiter=\"\\t\",\n",
    "                        low_memory=False, names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65bea19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter only new transcripts\n",
    "bambu_ref = bambu_ref.loc[bambu_ref[\"other\"].str.contains('transcript_id \"BambuTx')]\n",
    "\n",
    "\n",
    "## Filter only protein coding transcripts\n",
    "original_ref = original_ref.loc[original_ref[\"other\"].str.contains('transcript_biotype \"protein_coding\";')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6ef517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save files\n",
    "bambu_ref.to_csv(\"../../data/processed/website_annotations/only_new_transcripts_no_filter.gtf\", sep=\"\\t\", \n",
    "                index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "original_ref.to_csv(\"../../data/processed/website_annotations/only_protein_coding_transcripts_no_filter.gtf\", sep=\"\\t\", \n",
    "                index=False, header=False, quoting=csv.QUOTE_NONE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
